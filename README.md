# ğŸ“š Uncensored LLM Book + Audio Factory

> **Generate complete books in minutes.** Choose a topic. Get Markdown, PDF, MP3, and M4B audiobooksâ€”all generated privately with an uncensored LLM running in a sandbox.

This is an **AI-first native application**. Unlike traditional apps that bolt AI features onto existing frameworks, this entire platform is built around what AI makes possible. Without AI, this wouldn't exist.

---

## âœ¨ What You Get

| Feature | Details |
|---------|---------|
| ğŸ“– **Full Book Generation** | Markdown + PDF + plain text, all auto-generated |
| ğŸµ **Audiobooks** | MP3 and M4B formats with local TTS |
| ğŸ¯ **Smart Queue** | Expand/collapse books to see subtasks; reorder freely |
| âš¡ **Job Controls** | Stop, cancel, resume, retry with one click |
| ğŸ¨ **Reader View** | Beautifully rendered Markdown in your browser |
| ğŸ’¡ **Smart Suggestions** | Get topic recommendations based on your history |
| ğŸ”’ **Fully Private** | Runs locally; your model never leaves your machine |
| ğŸ³ **Docker Ready** | One command to run everything |

---

## ğŸš€ Quick Start

### Local Development (5 minutes)

```bash
# Clone and setup
git clone <repo>
cd Book_Generator

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
cp .env.example .env

# Start the app
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Open **http://localhost:8000** in your browser.

### Docker (Recommended)

```bash
# One command does everything
docker-compose up -d --build

# App is ready at http://localhost:8600
```

The container includes:
- ğŸ FastAPI web server
- ğŸ¦™ Ollama with local LLM
- ğŸ“¦ All dependencies (ffmpeg, TTS, PDF tools)
- ğŸ“ Persistent data volumes

---

## ğŸ—ï¸ Architecture

**Tech Stack:**
- **Backend:** FastAPI + Jinja2 + HTMX
- **Database:** SQLite with async support (aiosqlite)
- **LLM:** Ollama (100% local, fully private)
- **Audio:** Coqui TTS + ffmpeg
- **Export:** WeasyPrint + xhtml2pdf for PDFs
- **Orchestration:** Docker Compose

---

## ğŸ“– Features in Detail

### Create Books
Simply enter a topic. The app generates:
- âœ… Structured Markdown with headers, sections, and formatting
- âœ… Rendered PDF for print and sharing
- âœ… Plain text for audio generation
- âœ… Full MP3 and M4B audiobooks (optionally)

### Intelligent Queue Management
- **Hierarchical display:** Books appear as expandable sections
- **Click to expand:** See all constituent subtasks (PDF, MP3, M4B)
- **Drag-free reordering:** Up/Down buttons move books; subtasks stay together
- **Live ETA:** See how long until your book is done
- **Job controls:** Stop, cancel, pause, or resume at any time

### Read & Listen
- **In-app reader:** Beautifully formatted, distraction-free reading
- **Local playback:** Listen with adjustable voices and speeds
- **Multiple formats:** MP3 for mobile, M4B for audiobook apps

### Private & Secure
- Model runs in a sandbox container
- Cannot access your files or internet
- All processing happens locally
- No data sent anywhere

---

## ğŸ¯ How It Works

### Book Generation Flow
1. **Create:** Enter topic â†’ app queues generation job
2. **Generate:** LLM creates full book in Markdown
3. **Export:** Simultaneously queue PDF, MP3, M4B jobs
4. **Assemble:** Return finished assets (Markdown, PDF, audio files)

### Queue Organization
Books are the primary unit. When you create a book, four jobs are automatically queued:
```
[Book] "Advanced Physics" â† Your main job (can reorder)
  â”œâ”€ PDF Export
  â”œâ”€ MP3 Audiobook
  â””â”€ M4B Audiobook
```

Only the book can be reordered. Subtasks inherit dependencies and always follow.

### Storage
- **Database:** `data/app.db` (SQLite with job metadata)
- **Generated books:** `data/jobs/{job_id}/book.md`
- **Assets:** Plain text, PDF, MP3, M4B all organized per job
- **Persistence:** Everything persists across restarts

---

## âš™ï¸ Configuration

### Environment Variables
Create a `.env` file (or set via exports):

```bash
# Model configuration
export OLLAMA_BASE_URL="http://127.0.0.1:11434"
export OLLAMA_MODEL="huihui_ai/llama3.2-abliterate:3b"
export OLLAMA_AUTO_PULL=true

# TTS configuration
export LOCAL_TTS_MODEL="tts_models/en/vctk/vits"
export LOCAL_TTS_DEFAULT_VOICE="p225"

# API timeouts
export REQUEST_TIMEOUT_SECONDS="120"
```

### Switching Models
Change `OLLAMA_MODEL` in `.env` or set via environment. The app auto-pulls on startup if missing.

### TTS Voices
Coqui TTS downloads on first use. Speaker IDs vary by model:
- `p225`, `p229`, `p230` for VCTK model
- Try different voices to find your favorite

---

## ğŸ› ï¸ Development

### Run Tests
```bash
# All tests with coverage
pytest

# Specific test file
pytest tests/test_queue_expansion.py -v

# Run with output
pytest -s
```

**Test Coverage:**
- 22 comprehensive tests covering queue, jobs, export, TTS
- Tests for the new expandable queue feature
- Integration tests with the FastAPI test client

### Code Quality
```bash
# Format code (Ruff)
python -m ruff format .
python -m ruff check .

# Or use Black + Pylint
python -m black .
python -m pylint app tests
```

### Server Management
```bash
./scripts/server.sh start     # Start uvicorn in background
./scripts/server.sh stop      # Stop the server
./scripts/server.sh restart   # Restart the server
```

Logs are written to `.run/uvicorn.log`.

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app, endpoints
â”‚   â”œâ”€â”€ db.py                # SQLite operations, queries
â”‚   â”œâ”€â”€ generator.py         # Book generation logic
â”‚   â”œâ”€â”€ local_tts.py         # Audio synthesis
â”‚   â”œâ”€â”€ ollama_client.py     # LLM integration
â”‚   â”œâ”€â”€ pdf_export.py        # PDF rendering
â”‚   â”œâ”€â”€ eta.py               # ETA calculations
â”‚   â””â”€â”€ templates/           # Jinja2 HTML templates
â”‚       â”œâ”€â”€ index.html       # Main page (Create/Queue/Library)
â”‚       â”œâ”€â”€ job_detail.html  # Individual job view
â”‚       â””â”€â”€ partials/        # HTMX fragments
â”œâ”€â”€ tests/                   # Pytest test suite
â”‚   â”œâ”€â”€ test_queue_expansion.py
â”‚   â”œâ”€â”€ test_job_controls.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docker-compose.yml       # Ollama + app services
â”œâ”€â”€ Dockerfile               # App container
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ dev-requirements.txt     # Dev tools (pytest, ruff, etc.)
```

---

## ğŸ”’ Security & Privacy

- **Fully sandboxed:** LLM runs in Docker; cannot access host files
- **No cloud calls:** Everything is local and private
- **Data protection:** `.env` and `data/` ignored in git
- **Secrets hygiene:** Never commit real `.env` files
- **Open source:** Full transparency; audit the code

---

## ğŸ“ API Endpoints (Reference)

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Main UI (Create, Queue, Library) |
| `/jobs` | POST | Create new book job |
| `/jobs/{id}` | GET | View job details |
| `/jobs/{id}/read` | GET | Render book in browser |
| `/jobs/{id}/download.pdf` | GET | Download as PDF |
| `/jobs/{id}/download.txt` | GET | Download as text |
| `/jobs/{id}/audiobook` | GET | Stream MP3/M4B audio |
| `/jobs/{id}/move` | POST | Reorder in queue |
| `/jobs/{id}/stop` | POST | Pause running job |
| `/jobs/{id}/cancel` | POST | Cancel job |
| `/jobs/{id}/resume` | POST | Resume paused job |
| `/jobs/{id}/retry` | POST | Retry failed job |

---

## ğŸ“ Tips & Tricks

- **Complex topics:** Break into multiple smaller topics for faster generation
- **Audiobook quality:** Choose VCTK speaker voices for better results
- **PDF formatting:** Markdown headings automatically become PDF sections
- **Batch creation:** Queue multiple topics; they'll process sequentially
- **Custom voices:** Swap TTS model for different language support

---

## ğŸ› Troubleshooting

**Model won't download?**
- Check `OLLAMA_BASE_URL` points to your Ollama instance
- Verify Ollama is running: `curl http://localhost:11434/api/tags`
- Set `OLLAMA_AUTO_PULL=true` to auto-download on startup

**Audio generation slow?**
- TTS downloads the model on first run (100MB+)
- Subsequent generations are instant
- Use smaller models if you have limited resources

**PDF export failing?**
- Ensure `weasyprint` dependencies are installed
- Check `ffmpeg` is available: `ffmpeg -version`
- Try xhtml2pdf fallback (built-in)

**Docker volume issues?**
- Ensure `data/` directory exists and is writable
- Check Docker has permission to mount volumes
- Try `docker system prune` to clean up

---

## ğŸ“œ License

Check LICENSE file. Built with â¤ï¸ for the open-source community.

---

## ğŸŒŸ Contributing

Pull requests welcome! Please:
1. Fork the repo
2. Create a feature branch
3. Add tests for new functionality
4. Submit PR with clear description

---

**Made with ğŸ¤– AI + ğŸ’» code**
